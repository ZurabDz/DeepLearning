{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import lltm_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLTM(torch.nn.Module):\n",
    "    def __init__(self, input_features, state_size):\n",
    "        super(LLTM, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.state_size = state_size\n",
    "        # 3 * state_size for input gate, output gate and candidate cell gate.\n",
    "        # input_features + state_size because we will multiply with [input, h].\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.empty(3 * state_size, input_features + state_size))\n",
    "        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.state_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, +stdv)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        old_h, old_cell = state\n",
    "        X = torch.cat([old_h, input], dim=1)\n",
    "\n",
    "        # Compute the input, output and candidate cell gates with one MM.\n",
    "        gate_weights = F.linear(X, self.weights, self.bias)\n",
    "        # Split the combined gate weight matrix into its components.\n",
    "        gates = gate_weights.chunk(3, dim=1)\n",
    "\n",
    "        input_gate = torch.sigmoid(gates[0])\n",
    "        output_gate = torch.sigmoid(gates[1])\n",
    "        # Here we use an ELU instead of the usual tanh.\n",
    "        candidate_cell = F.elu(gates[2])\n",
    "\n",
    "        # Compute the new cell state.\n",
    "        new_cell = old_cell + candidate_cell * input_gate\n",
    "        # Compute the new hidden state and output.\n",
    "        new_h = torch.tanh(new_cell) * output_gate\n",
    "\n",
    "        return new_h, new_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "input_features = 10\n",
    "state_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(batch_size, input_features).to('cuda')\n",
    "h = torch.randn(batch_size, state_size).to('cuda')\n",
    "C = torch.randn(batch_size, state_size).to('cuda')\n",
    "\n",
    "rnn = LLTM(input_features, state_size).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_h, new_C = rnn(X, (h, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100000)):\n",
    "    new_h, new_C = rnn(X, (h, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "# Our module!\n",
    "import lltm_cpp\n",
    "\n",
    "class LLTMFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, weights, bias, old_h, old_cell):\n",
    "        outputs = lltm_cpp.forward(input, weights, bias, old_h, old_cell)\n",
    "        new_h, new_cell = outputs[:2]\n",
    "        variables = outputs[1:] + [weights]\n",
    "        # ctx.save_for_backward(*variables)\n",
    "\n",
    "        return new_h, new_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLTM(torch.nn.Module):\n",
    "    def __init__(self, input_features, state_size):\n",
    "        super(LLTM, self).__init__()\n",
    "        self.input_features = input_features\n",
    "        self.state_size = state_size\n",
    "        self.weights = torch.nn.Parameter(\n",
    "            torch.empty(3 * state_size, input_features + state_size))\n",
    "        self.bias = torch.nn.Parameter(torch.empty(3 * state_size))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.state_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, +stdv)\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        return LLTMFunction.apply(input, self.weights, self.bias, *state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 16\n",
    "input_features = 32\n",
    "state_size = 128\n",
    "\n",
    "X = torch.randn(batch_size, input_features).to('cuda')\n",
    "h = torch.randn(batch_size, state_size).to('cuda')\n",
    "C = torch.randn(batch_size, state_size).to('cuda')\n",
    "\n",
    "rnn = LLTM(input_features, state_size).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in tqdm(range(100000)):\n",
    "    new_h, new_C = rnn(X, (h, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error building extension 'lltm_scpp': [1/3] /home/archangel/miniconda3/envs/jax_wav2vec2/bin/nvcc  -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++14 -c /home/archangel/DeepLearning/research/lltm_cuda_kernel.cu -o lltm_cuda_kernel.cuda.o \nFAILED: lltm_cuda_kernel.cuda.o \n/home/archangel/miniconda3/envs/jax_wav2vec2/bin/nvcc  -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++14 -c /home/archangel/DeepLearning/research/lltm_cuda_kernel.cu -o lltm_cuda_kernel.cuda.o \n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: identifier \"lltm_cuda_forward_kernel\" is undefined\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: type name is not allowed\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: expected an expression\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: identifier \"lltm_cuda_forward_kernel\" is undefined\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: type name is not allowed\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: expected an expression\n\n6 errors detected in the compilation of \"/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu\".\n[2/3] c++ -MMD -MF lltm.o.d -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /home/archangel/DeepLearning/research/lltm.cpp -o lltm.o \nninja: build stopped: subcommand failed.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1808\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1807\u001b[0m     stdout_fileno \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1808\u001b[0m     subprocess\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m   1809\u001b[0m         command,\n\u001b[1;32m   1810\u001b[0m         stdout\u001b[39m=\u001b[39;49mstdout_fileno \u001b[39mif\u001b[39;49;00m verbose \u001b[39melse\u001b[39;49;00m subprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m   1811\u001b[0m         stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mSTDOUT,\n\u001b[1;32m   1812\u001b[0m         cwd\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1813\u001b[0m         check\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1814\u001b[0m         env\u001b[39m=\u001b[39;49menv)\n\u001b[1;32m   1815\u001b[0m \u001b[39mexcept\u001b[39;00m subprocess\u001b[39m.\u001b[39mCalledProcessError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1816\u001b[0m     \u001b[39m# Python 2 and 3 compatible way of getting the error object.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/subprocess.py:516\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m     \u001b[39mif\u001b[39;00m check \u001b[39mand\u001b[39;00m retcode:\n\u001b[0;32m--> 516\u001b[0m         \u001b[39mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[39m.\u001b[39margs,\n\u001b[1;32m    517\u001b[0m                                  output\u001b[39m=\u001b[39mstdout, stderr\u001b[39m=\u001b[39mstderr)\n\u001b[1;32m    518\u001b[0m \u001b[39mreturn\u001b[39;00m CompletedProcess(process\u001b[39m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['ninja', '-v']' returned non-zero exit status 1.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lltm_cpp \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlltm_scpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlltm.cpp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlltm_cuda_kernel.cu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1202\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(name,\n\u001b[1;32m   1112\u001b[0m          sources: Union[\u001b[39mstr\u001b[39m, List[\u001b[39mstr\u001b[39m]],\n\u001b[1;32m   1113\u001b[0m          extra_cflags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1121\u001b[0m          is_standalone\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1122\u001b[0m          keep_intermediates\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[39m    Loads a PyTorch C++ extension just-in-time (JIT).\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[39m                verbose=True)\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m-> 1202\u001b[0m     \u001b[39mreturn\u001b[39;00m _jit_compile(\n\u001b[1;32m   1203\u001b[0m         name,\n\u001b[1;32m   1204\u001b[0m         [sources] \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(sources, \u001b[39mstr\u001b[39;49m) \u001b[39melse\u001b[39;49;00m sources,\n\u001b[1;32m   1205\u001b[0m         extra_cflags,\n\u001b[1;32m   1206\u001b[0m         extra_cuda_cflags,\n\u001b[1;32m   1207\u001b[0m         extra_ldflags,\n\u001b[1;32m   1208\u001b[0m         extra_include_paths,\n\u001b[1;32m   1209\u001b[0m         build_directory \u001b[39mor\u001b[39;49;00m _get_build_directory(name, verbose),\n\u001b[1;32m   1210\u001b[0m         verbose,\n\u001b[1;32m   1211\u001b[0m         with_cuda,\n\u001b[1;32m   1212\u001b[0m         is_python_module,\n\u001b[1;32m   1213\u001b[0m         is_standalone,\n\u001b[1;32m   1214\u001b[0m         keep_intermediates\u001b[39m=\u001b[39;49mkeep_intermediates)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1425\u001b[0m, in \u001b[0;36m_jit_compile\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_python_module, is_standalone, keep_intermediates)\u001b[0m\n\u001b[1;32m   1421\u001b[0m                 hipified_sources\u001b[39m.\u001b[39madd(hipify_result[s_abs][\u001b[39m\"\u001b[39m\u001b[39mhipified_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m s_abs \u001b[39min\u001b[39;00m hipify_result \u001b[39melse\u001b[39;00m s_abs)\n\u001b[1;32m   1423\u001b[0m             sources \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(hipified_sources)\n\u001b[0;32m-> 1425\u001b[0m         _write_ninja_file_and_build_library(\n\u001b[1;32m   1426\u001b[0m             name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m   1427\u001b[0m             sources\u001b[39m=\u001b[39;49msources,\n\u001b[1;32m   1428\u001b[0m             extra_cflags\u001b[39m=\u001b[39;49mextra_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1429\u001b[0m             extra_cuda_cflags\u001b[39m=\u001b[39;49mextra_cuda_cflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1430\u001b[0m             extra_ldflags\u001b[39m=\u001b[39;49mextra_ldflags \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1431\u001b[0m             extra_include_paths\u001b[39m=\u001b[39;49mextra_include_paths \u001b[39mor\u001b[39;49;00m [],\n\u001b[1;32m   1432\u001b[0m             build_directory\u001b[39m=\u001b[39;49mbuild_directory,\n\u001b[1;32m   1433\u001b[0m             verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1434\u001b[0m             with_cuda\u001b[39m=\u001b[39;49mwith_cuda,\n\u001b[1;32m   1435\u001b[0m             is_standalone\u001b[39m=\u001b[39;49mis_standalone)\n\u001b[1;32m   1436\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1437\u001b[0m     baton\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1537\u001b[0m, in \u001b[0;36m_write_ninja_file_and_build_library\u001b[0;34m(name, sources, extra_cflags, extra_cuda_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, is_standalone)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[1;32m   1536\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBuilding extension module \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 1537\u001b[0m _run_ninja_build(\n\u001b[1;32m   1538\u001b[0m     build_directory,\n\u001b[1;32m   1539\u001b[0m     verbose,\n\u001b[1;32m   1540\u001b[0m     error_prefix\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mError building extension \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1824\u001b[0m, in \u001b[0;36m_run_ninja_build\u001b[0;34m(build_directory, verbose, error_prefix)\u001b[0m\n\u001b[1;32m   1822\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(error, \u001b[39m'\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m error\u001b[39m.\u001b[39moutput:  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1823\u001b[0m     message \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00merror\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39mdecode(\u001b[39m*\u001b[39mSUBPROCESS_DECODE_ARGS)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m-> 1824\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error building extension 'lltm_scpp': [1/3] /home/archangel/miniconda3/envs/jax_wav2vec2/bin/nvcc  -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++14 -c /home/archangel/DeepLearning/research/lltm_cuda_kernel.cu -o lltm_cuda_kernel.cuda.o \nFAILED: lltm_cuda_kernel.cuda.o \n/home/archangel/miniconda3/envs/jax_wav2vec2/bin/nvcc  -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -std=c++14 -c /home/archangel/DeepLearning/research/lltm_cuda_kernel.cu -o lltm_cuda_kernel.cuda.o \n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: identifier \"lltm_cuda_forward_kernel\" is undefined\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: type name is not allowed\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: expected an expression\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: identifier \"lltm_cuda_forward_kernel\" is undefined\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: type name is not allowed\n\n/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu(59): error: expected an expression\n\n6 errors detected in the compilation of \"/home/archangel/DeepLearning/research/lltm_cuda_kernel.cu\".\n[2/3] c++ -MMD -MF lltm.o.d -DTORCH_EXTENSION_NAME=lltm_scpp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/TH -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/lib/python3.8/site-packages/torch/include/THC -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include -isystem /home/archangel/miniconda3/envs/jax_wav2vec2/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -c /home/archangel/DeepLearning/research/lltm.cpp -o lltm.o \nninja: build stopped: subcommand failed.\n"
     ]
    }
   ],
   "source": [
    "lltm_cpp = load(name='lltm_scpp', sources=['lltm.cpp', 'lltm_cuda_kernel.cu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('jax_wav2vec2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f06c4421131517ada4f17c4e5bfd42503021249cc0c01c2fe57a46d7416a6193"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
