{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForMaskedLM, AlbertConfig\n",
    "from transformers import AlbertTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "# tokenizer vocab size is 30_000 Model's torch.embedding can be index at max embedding_dim-1 (embedding_dim is model's vocabsize hence 30_001)\n",
    "vocab_size = 30_001\n",
    "truncate_longer_samples = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what's wrong with rust that creates infinite recursion\n",
    "# def _convert_token_to_id_with_added_voc(self, token: str) -> int:\n",
    "#         index = self._tokenizer.token_to_id(token)\n",
    "#         if index is None:\n",
    "#             # return self.unk_token_id\n",
    "#             return self.vocab_size\n",
    "#         return index\n",
    "\n",
    "tokenizer = AlbertTokenizerFast(tokenizer_file='tokenizer.json', model_max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_with_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed with truncation\"\"\"\n",
    "  return tokenizer(examples['text'], truncation=True, padding=\"max_length\",\n",
    "                   max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "  \"\"\"Mapping function to tokenize the sentences passed without truncation\"\"\"\n",
    "  return tokenizer(examples['text'], return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AlbertConfig.from_pretrained('albert-base-v2')\n",
    "config.vocab_size=vocab_size\n",
    "model = AlbertForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('text', data_files={'train': ['wikitext-103-v1/wikitext-103/wiki.train.tokens'],\n",
    " 'valid': ['wikitext-103-v1/wikitext-103/wiki.valid.tokens']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].map(encode, batched=True, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"valid\"].map(encode, batched=True, num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if truncate_longer_samples:\n",
    "  # remove other columns and set input_ids and attention_mask as PyTorch tensors\n",
    "  train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "  test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "else:\n",
    "  # remove other columns, and remain them as Python lists\n",
    "  test_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])\n",
    "  train_dataset.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of\n",
    "# max_seq_length.\n",
    "# grabbed from: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + max_length] for i in range(0, total_length, max_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not truncate_longer_samples:\n",
    "  train_dataset = train_dataset.map(group_texts, batched=True,\n",
    "                                    desc=f\"Grouping texts in chunks of {max_length}\", num_proc=8)\n",
    "  test_dataset = test_dataset.map(group_texts, batched=True,\n",
    "                                  desc=f\"Grouping texts in chunks of {max_length}\", num_proc=8)\n",
    "  # convert them from lists to torch tensors\n",
    "  train_dataset.set_format(\"torch\")\n",
    "  test_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='output',          \n",
    "    evaluation_strategy=\"steps\",    \n",
    "    overwrite_output_dir=True,      \n",
    "    num_train_epochs=60,            \n",
    "    per_device_train_batch_size=4 , \n",
    "    gradient_accumulation_steps=8,  \n",
    "    per_device_eval_batch_size=6,  \n",
    "    logging_steps=500,             \n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,  \n",
    "    save_total_limit=8,\n",
    "    # no_cuda=True\n",
    "    fp16=True,\n",
    "    fp16_opt_level='O2'\n",
    "    # half_precision_backend='apex'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a0d47d3aa5b0c9d2bc691111433f12583ff7ff367b00112319412551cbc7096f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
